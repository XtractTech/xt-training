{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from xt_training import metrics\n",
    "from xt_training.utils import (DummyOptimizer, SKDataset, SKInterface,\n",
    "                               functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/angran/GIT/msg\n"
     ]
    }
   ],
   "source": [
    "cd ../msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angran/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "from utils import transforms as xt_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.eager_outputs = lambda l, d: (l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = RCNNDataset(\n",
    "    root='/home/angran/Documents/datasets/PennFudanPed', \n",
    "    transforms=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "test_ds = RCNNDataset(\n",
    "    root='/home/angran/Documents/datasets/PennFudanPed', \n",
    "    transforms=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_ds = torch.utils.data.Subset(dataset, indices[:100])\n",
    "valid_ds = torch.utils.data.Subset(dataset, indices[100:120])\n",
    "test_ds = torch.utils.data.Subset(test_ds, indices[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (image, target)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and target is a dictionary of tensor values\n",
    "    \"\"\"\n",
    "    images, targets = zip(*data)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define training and validation data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_loaders = {}\n",
    "test_loaders['test'] = DataLoader(\n",
    "    test_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 4])\n",
      "1 torch.Size([2, 4])\n",
      "2 torch.Size([1, 4])\n",
      "3 torch.Size([3, 4])\n",
      "4 torch.Size([1, 4])\n",
      "5 torch.Size([1, 4])\n",
      "6 torch.Size([7, 4])\n",
      "7 torch.Size([7, 4])\n",
      "8 torch.Size([3, 4])\n",
      "9 torch.Size([6, 4])\n",
      "10 torch.Size([1, 4])\n",
      "11 torch.Size([4, 4])\n",
      "12 torch.Size([2, 4])\n",
      "13 torch.Size([2, 4])\n",
      "14 torch.Size([3, 4])\n",
      "15 torch.Size([3, 4])\n",
      "16 torch.Size([1, 4])\n",
      "17 torch.Size([2, 4])\n",
      "18 torch.Size([1, 4])\n",
      "19 torch.Size([1, 4])\n",
      "20 torch.Size([3, 4])\n",
      "21 torch.Size([5, 4])\n",
      "22 torch.Size([3, 4])\n",
      "23 torch.Size([1, 4])\n",
      "24 torch.Size([1, 4])\n",
      "25 torch.Size([5, 4])\n",
      "26 torch.Size([3, 4])\n",
      "27 torch.Size([1, 4])\n",
      "28 torch.Size([2, 4])\n",
      "29 torch.Size([1, 4])\n",
      "30 torch.Size([2, 4])\n",
      "31 torch.Size([2, 4])\n",
      "32 torch.Size([6, 4])\n",
      "33 torch.Size([3, 4])\n",
      "34 torch.Size([2, 4])\n",
      "35 torch.Size([1, 4])\n",
      "36 torch.Size([3, 4])\n",
      "37 torch.Size([1, 4])\n",
      "38 torch.Size([1, 4])\n",
      "39 torch.Size([1, 4])\n",
      "40 torch.Size([3, 4])\n",
      "41 torch.Size([4, 4])\n",
      "42 torch.Size([1, 4])\n",
      "43 torch.Size([1, 4])\n",
      "44 torch.Size([2, 4])\n",
      "45 torch.Size([2, 4])\n",
      "46 torch.Size([1, 4])\n",
      "47 torch.Size([3, 4])\n",
      "48 torch.Size([3, 4])\n",
      "49 torch.Size([1, 4])\n",
      "50 torch.Size([2, 4])\n",
      "51 torch.Size([1, 4])\n",
      "52 torch.Size([2, 4])\n",
      "53 torch.Size([1, 4])\n",
      "54 torch.Size([3, 4])\n",
      "55 torch.Size([6, 4])\n",
      "56 torch.Size([3, 4])\n",
      "57 torch.Size([5, 4])\n",
      "58 torch.Size([2, 4])\n",
      "59 torch.Size([1, 4])\n",
      "60 torch.Size([7, 4])\n",
      "61 torch.Size([2, 4])\n",
      "62 torch.Size([4, 4])\n",
      "63 torch.Size([2, 4])\n",
      "64 torch.Size([1, 4])\n",
      "65 torch.Size([1, 4])\n",
      "66 torch.Size([2, 4])\n",
      "67 torch.Size([7, 4])\n",
      "68 torch.Size([1, 4])\n",
      "69 torch.Size([6, 4])\n",
      "70 torch.Size([1, 4])\n",
      "71 torch.Size([1, 4])\n",
      "72 torch.Size([3, 4])\n",
      "73 torch.Size([4, 4])\n",
      "74 torch.Size([4, 4])\n",
      "75 torch.Size([1, 4])\n",
      "76 torch.Size([5, 4])\n",
      "77 torch.Size([3, 4])\n",
      "78 torch.Size([3, 4])\n",
      "79 torch.Size([3, 4])\n",
      "80 torch.Size([5, 4])\n",
      "81 torch.Size([2, 4])\n",
      "82 torch.Size([2, 4])\n",
      "83 torch.Size([4, 4])\n",
      "84 torch.Size([3, 4])\n",
      "85 torch.Size([2, 4])\n",
      "86 torch.Size([3, 4])\n",
      "87 torch.Size([4, 4])\n",
      "88 torch.Size([1, 4])\n",
      "89 torch.Size([2, 4])\n",
      "90 torch.Size([6, 4])\n",
      "91 torch.Size([2, 4])\n",
      "92 torch.Size([1, 4])\n",
      "93 torch.Size([2, 4])\n",
      "94 torch.Size([4, 4])\n",
      "95 torch.Size([1, 4])\n",
      "96 torch.Size([2, 4])\n",
      "97 torch.Size([3, 4])\n",
      "98 torch.Size([4, 4])\n",
      "99 torch.Size([4, 4])\n",
      "100 torch.Size([2, 4])\n",
      "101 torch.Size([1, 4])\n",
      "102 torch.Size([2, 4])\n",
      "103 torch.Size([2, 4])\n",
      "104 torch.Size([3, 4])\n",
      "105 torch.Size([3, 4])\n",
      "106 torch.Size([2, 4])\n",
      "107 torch.Size([3, 4])\n",
      "108 torch.Size([2, 4])\n",
      "109 torch.Size([6, 4])\n",
      "110 torch.Size([2, 4])\n",
      "111 torch.Size([3, 4])\n",
      "112 torch.Size([3, 4])\n",
      "113 torch.Size([2, 4])\n",
      "114 torch.Size([2, 4])\n",
      "115 torch.Size([1, 4])\n",
      "116 torch.Size([2, 4])\n",
      "117 torch.Size([7, 4])\n",
      "118 torch.Size([3, 4])\n",
      "119 torch.Size([7, 4])\n",
      "120 torch.Size([3, 4])\n",
      "121 torch.Size([4, 4])\n",
      "122 torch.Size([6, 4])\n",
      "123 torch.Size([2, 4])\n",
      "124 torch.Size([6, 4])\n",
      "125 torch.Size([2, 4])\n",
      "126 torch.Size([3, 4])\n",
      "127 torch.Size([1, 4])\n",
      "128 torch.Size([1, 4])\n",
      "129 torch.Size([1, 4])\n",
      "130 torch.Size([1, 4])\n",
      "131 torch.Size([3, 4])\n",
      "132 torch.Size([3, 4])\n",
      "133 torch.Size([7, 4])\n",
      "134 torch.Size([2, 4])\n",
      "135 torch.Size([2, 4])\n",
      "136 torch.Size([7, 4])\n",
      "137 torch.Size([4, 4])\n",
      "138 torch.Size([2, 4])\n",
      "139 torch.Size([6, 4])\n",
      "140 torch.Size([1, 4])\n",
      "141 torch.Size([7, 4])\n",
      "142 torch.Size([1, 4])\n",
      "143 torch.Size([4, 4])\n",
      "144 torch.Size([3, 4])\n",
      "145 torch.Size([2, 4])\n",
      "146 torch.Size([2, 4])\n",
      "147 torch.Size([1, 4])\n",
      "148 torch.Size([1, 4])\n",
      "149 torch.Size([1, 4])\n",
      "150 torch.Size([2, 4])\n",
      "151 torch.Size([2, 4])\n",
      "152 torch.Size([2, 4])\n",
      "153 torch.Size([3, 4])\n",
      "154 torch.Size([4, 4])\n",
      "155 torch.Size([1, 4])\n",
      "156 torch.Size([3, 4])\n",
      "157 torch.Size([2, 4])\n",
      "158 torch.Size([1, 4])\n",
      "159 torch.Size([3, 4])\n",
      "160 torch.Size([1, 4])\n",
      "161 torch.Size([2, 4])\n",
      "162 torch.Size([3, 4])\n",
      "163 torch.Size([3, 4])\n",
      "164 torch.Size([3, 4])\n",
      "165 torch.Size([4, 4])\n",
      "166 torch.Size([1, 4])\n",
      "167 torch.Size([3, 4])\n",
      "168 torch.Size([1, 4])\n",
      "169 torch.Size([3, 4])\n",
      "170 torch.Size([2, 4])\n",
      "171 torch.Size([4, 4])\n",
      "172 torch.Size([2, 4])\n",
      "173 torch.Size([1, 4])\n",
      "174 torch.Size([1, 4])\n",
      "175 torch.Size([2, 4])\n",
      "176 torch.Size([2, 4])\n",
      "177 torch.Size([4, 4])\n",
      "178 torch.Size([2, 4])\n",
      "179 torch.Size([1, 4])\n",
      "180 torch.Size([3, 4])\n",
      "181 torch.Size([3, 4])\n",
      "182 torch.Size([1, 4])\n",
      "183 torch.Size([4, 4])\n",
      "184 torch.Size([2, 4])\n",
      "185 torch.Size([2, 4])\n",
      "186 torch.Size([3, 4])\n",
      "187 torch.Size([3, 4])\n",
      "188 torch.Size([1, 4])\n",
      "189 torch.Size([1, 4])\n",
      "190 torch.Size([1, 4])\n",
      "191 torch.Size([7, 4])\n",
      "192 torch.Size([2, 4])\n",
      "193 torch.Size([3, 4])\n",
      "194 torch.Size([4, 4])\n",
      "195 torch.Size([1, 4])\n",
      "196 torch.Size([1, 4])\n",
      "197 torch.Size([3, 4])\n",
      "198 torch.Size([1, 4])\n",
      "199 torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(i, next(iter(train_loader), None)[1][0]['boxes'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-47f6a1d00e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_0 = train_loader.dataset[1][0]\n",
    "# y_0 = train_loader.dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, y_pred = model(torch.unsqueeze(x_0, 0), targets=[y_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, (x, y) in enumerate(train_loader):\n",
    "#     print(x.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODHandler:\n",
    "    def __init__(self, callable, default=None):\n",
    "        self.fn = callable\n",
    "        self.default = default\n",
    "    def __call__(self, inputs, y):\n",
    "        try:\n",
    "            return self.fn(inputs, y)\n",
    "        except:\n",
    "            return self.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = ODHandler(lambda loss, y_pred: sum(l for l in loss.values()), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = lambda loss, y_pred: sum(l for l in loss.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to the right device\n",
    "# model.to(device) # Already implemented in xt-training\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(valid_loader), len(test_loaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "\n",
      "\n",
      "Initial\n",
      "----------\n",
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angran/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c0dee511fd68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_loaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     device='cuda:0',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#     eval_metrics=eval_metrics,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     on_exit=default_train_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/xt-training/xt_training/utils/functional.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(save_dir, train_loader, model, optimizer, epochs, loss_fn, starting_loss, overwrite, val_loader, test_loaders, device, scheduler, is_batch_scheduler, eval_metrics, tokenizer, on_exit, use_nni)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloader_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstarting_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstarting_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GIT/xt-training/xt_training/runner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loader, mode, return_preds)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;31m# loss_batch = loss_fn(y_pred, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m### for object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# y_pred is an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[1;32m    104\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mresize_boxes\u001b[0;34m(boxes, original_size, new_size)\u001b[0m\n\u001b[1;32m    272\u001b[0m     ]\n\u001b[1;32m    273\u001b[0m     \u001b[0mratio_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "save_dir = '/home/angran/Downloads/test/'\n",
    "\n",
    "# stats, matrix = \n",
    "functional.train(\n",
    "    save_dir=save_dir,\n",
    "    train_loader=train_loader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=2,\n",
    "    loss_fn=loss_fn,\n",
    "    overwrite=True,\n",
    "    val_loader=valid_loader,\n",
    "    test_loaders=test_loaders,\n",
    "#     device='cuda:0',\n",
    "    scheduler=lr_scheduler,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     on_exit=default_train_exit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn\n",
    ")\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "for i_batch, (x, y) in enumerate(train_loader):\n",
    "    if isinstance(y, Iterable):\n",
    "        print(type(y))\n",
    "        print(y['boxes'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[0][1]['boxes'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
